import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import re

# --- é…ç½®è·¯å¾„ (æ ¹æ®ä½ çš„ trainfull.py ä¿®æ”¹) ---
# åŸºç¡€æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = "/home/yfjin/ClientStimul/Qwen2.5-7B-Instruct"
# GRPO è®­ç»ƒåçš„æœ€ç»ˆæ¨¡å‹è·¯å¾„ (trainfull.py ä¸­ä¿å­˜çš„ä½ç½®)
ADAPTER_PATH = "/home/yfjin/ClientStimul/trl/CS_grpo_new_fullapi/checkpoint-1800"

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="ClientStimul æ¨¡æ‹Ÿæ¥è®¿è€…æµ‹è¯•", layout="wide")

# --- ä¾§è¾¹æ ï¼šé…ç½®ä¸ç”»åƒ ---
with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®")


    
    st.divider()
    
    st.subheader("ç”Ÿæˆå‚æ•°")
    temperature = st.slider("Temperature", 0.0, 2.0, 0.8)
    max_new_tokens = st.slider("Max New Tokens", 128, 1024, 512)
    top_p = st.slider("Top P", 0.0, 1.0, 0.9)

    st.divider()
    
    st.subheader("ğŸ‘¤ ç”¨æˆ·ç”»åƒ (Persona)")
    # é»˜è®¤ç”»åƒæ¥è‡ªä½ çš„ prompt.py
    default_persona = {
        "background": "ä¸­æ–‡æ¯è¯­çš„å¹´è½»æ±‚èŒè€…ï¼Œæ›¾æœ‰å·¥ä½œç»å†ï¼Œæ€§æ ¼åå†…å‘ã€æ•æ„Ÿäºäººé™…æ‹’ç»ä¸â€œä¸¢è„¸â€ã€‚è¿‘æœŸæŒç»­æ±‚èŒæœªè·offerï¼Œåœ¨å®è§‚å°±ä¸šä¸æ™¯æ°”çš„èƒŒæ™¯ä¸‹è§¦å‘å¼ºçƒˆç„¦è™‘ä¸è‡ªæˆ‘æ€€ç–‘ã€‚ä¸ºæå‡é¢è¯•è¡¨ç°æ›¾è¯·é¢è¯•è¾…å¯¼å¹¶å®Œæˆç¬¬ä¸€ç‰ˆææ–™ï¼Œä½†åœ¨éœ€è¦è¿›ä¸€æ­¥æŒ–æ˜ä¸è¡¨è¿°è‡ªèº«ç»å†æ—¶åŠ¨åŠ›ä¸‹é™ã€åœæ»ã€‚è¿‡å¾€é¢†å¯¼åé¦ˆå…¶äººé™…/èŒåœºç¤¾äº¤éœ€åŠ å¼ºï¼Œä¿ƒä½¿å…¶æ€€ç–‘è‡ªå·±æ˜¯å¦é€‚åˆèŒåœºæ–‡åŒ–ã€‚æ›¾çº¿ä¸‹åšè¿‡å¿ƒç†å’¨è¯¢ã€‚",
        "chief_complaint": "è¡¨é¢è¯‰æ±‚æ˜¯â€œæ‰¾ä¸åˆ°å·¥ä½œã€å¾ˆç„¦è™‘ã€æ‹…å¿ƒè¢«æ·˜æ±°â€ï¼Œæƒ³æå‡é¢è¯•ä¸æ±‚èŒæ•ˆæœã€ç¼“è§£ç„¦è™‘å¹¶æ¢å¤åŠ¨åŠ›ã€‚æ›´æ·±å±‚çš„æ˜¯è‡ªæˆ‘ä»·å€¼æ„Ÿå—å¤–ç•Œè¯„ä»·å¼ºçƒˆå½±å“ï¼Œå†…å‘ç‰¹è´¨åœ¨èŒåœºç¤¾äº¤æƒ…å¢ƒä¸­å¸¦æ¥è´Ÿå›é¦ˆï¼Œå¯¼è‡´å›é¿ä¸è‡ªæˆ‘å¦å®šå¾ªç¯ï¼›å¯¹åŠªåŠ›ä¸ç»“æœçš„å…³ç³»å­˜åœ¨æ‚²è§‚é¢„æœŸï¼Œå› ç¼ºå°‘å³æ—¶æ­£åé¦ˆè€Œéš¾ä»¥åšæŒã€‚",
        "cognitive_patterns": "å­˜åœ¨ç¾éš¾åŒ–ï¼ˆæ‹…å¿ƒè¢«æ·˜æ±°ï¼‰ã€è¿‡åº¦æ¦‚æ‹¬ï¼ˆæ²¡æ‹¿åˆ°offerå°±å½’å› ä¸ºè‡ªèº«ä¸è¡Œï¼‰ã€è¯»å¿ƒä¸æ ‡ç­¾åŒ–ï¼ˆç¤¾ä¼šä¸å–œæ¬¢å†…å‘ã€è‡ªå·±â€œä¸å¤Ÿä¼˜ç§€/ä¸é€‚åˆèŒåœºâ€ï¼‰ã€é€‰æ‹©æ€§æ³¨æ„ä¸è´¬ä½è‡ªèº«ï¼ˆçœ‹è§ä»–äººä¼˜ç§€å’ŒåŠªåŠ›ï¼Œå¿½è§†è‡ªèº«åŒ¹é…åº¦ä¸å·²æœ‰å°è¯•ï¼‰ã€ç»“æœé¢„è¨€ä¸å¤–æ§å€¾å‘ï¼ˆç»“æœé è¿æ°”ï¼ŒåŠªåŠ›ä¹Ÿå¯èƒ½æ— ç”¨ï¼‰ã€éæ­¤å³å½¼/æ¡ä»¶å¼ä¿¡å¿µï¼ˆä¸å¤–å‘å°±éš¾æˆåŠŸï¼›æ²¡æœ‰å¤–ç•Œæ­£åé¦ˆå°±éš¾ä»¥ç»§ç»­ï¼‰ã€‚æ ¸å¿ƒä¿¡å¿µå€¾å‘äºâ€œæˆ‘ä¸å¤Ÿå¥½/ä¸è¢«çœ‹è§â€â€œä¸–ç•Œå¾ˆç«äº‰ä¸”è‹›åˆ»â€ï¼Œæ¡ä»¶å‡è®¾ä¸ºâ€œè‹¥ä¸»åŠ¨ç¤¾äº¤è¢«æ‹’å°±å¾ˆä¸¢è„¸ï¼Œè¯´æ˜æˆ‘ä¸åˆé€‚â€ã€‚",
        "emotional_behavioral": "ä¸»å¯¼æƒ…ç»ªï¼šç„¦è™‘ã€è‡ªæˆ‘æ€€ç–‘ã€ç¾è€»/å°´å°¬ã€ç¾¡æ…•ä»–äººã€æ— åŠ›ä¸æŒ«è´¥ï¼›åœ¨é€€å‡ºç¤¾äº¤ã€æˆä¸ºæ—è§‚è€…æ—¶æ„Ÿåˆ°æ”¾æ¾ä¸å®‰å…¨ã€‚è¡Œä¸ºä¸Šè¡¨ç°ä¸ºå›é¿ï¼ˆå‡å°‘ä¸»åŠ¨è¯·æ•™/ç¤¾äº¤ï¼‰ã€æ‹–å»¶ï¼ˆé¢è¯•ææ–™äºŒç¨¿åœæ»ï¼‰ã€è®¡åˆ’-æ‰§è¡Œè„±èŠ‚ï¼ˆå¤œæ™šè®¡åˆ’ã€ç™½å¤©ä¸è½å®ï¼‰ã€ç¤¾ä¼šæ¯”è¾ƒä¸åˆ·æ±‚èŒAppå¯»åŒæ¸©å±‚ã€åœ¨è´Ÿåé¦ˆæ—¶è‡ªæˆ‘æ‰“å‡»ï¼›åŒæ—¶ä¹Ÿæœ‰ç§¯ææ±‚åŠ©ï¼ˆè¾…å¯¼ã€å’¨è¯¢ï¼‰ã€æ„¿æ„å°è¯•è‡ªæˆ‘å¥–åŠ±ä¸å°æ­¥æš´éœ²çš„å€¾å‘ã€‚",
        "speech_style": "ç¤¼è²Œã€åˆä½œä¸”åæ€æ€§å¼ºï¼›èƒ½æ¸…æ™°æè¿°å†…åœ¨ä½“éªŒä¸å›°æƒ‘ï¼Œé€»è¾‘æ€§å¥½ï¼Œä¼šæå‡ºå…·ä½“é—®é¢˜ä¸ç±»æ¯”ï¼ˆå¥–åŠ±æœºåˆ¶æ¯”å–»ï¼‰ï¼›è¯­æ°”ä¸­æ€§åæ‚²è§‚ä½†å¼€æ”¾æ¥çº³å»ºè®®ï¼Œæ— æ˜æ˜¾å¯¹æŠ—ã€‚",
        "resistance_level": "ä½",
        "strengths_resources": "è‡ªçœèƒ½åŠ›å¼ºï¼Œèƒ½è¯†åˆ«â€œä¸å¤–ç•Œä¸€èµ·æ‰“å‡»è‡ªå·±â€çš„æ¨¡å¼ï¼›æ„¿æ„æ±‚åŠ©ä¸å­¦ä¹ ï¼ˆé¢è¯•è¾…å¯¼ã€å¿ƒç†å’¨è¯¢ï¼‰ï¼›èƒ½ç†è§£å¹¶æ¥å—å°æ­¥æ”¹å˜ä¸è‡ªæˆ‘å¼ºåŒ–çš„ç­–ç•¥ï¼›é‡è§†çœŸå®æ€§ä¸è¾¹ç•Œï¼Œå…·å¤‡ç¨³å®šçš„è‡ªæˆ‘å®‰æŠšæ–¹å¼ï¼ˆç‹¬å¤„ã€æ—è§‚è€…å§¿æ€ï¼‰ï¼›è§‚å¯ŸåŠ›ä¸æ€è€ƒæ·±åº¦å¯è½¬åŒ–ä¸ºèŒåœºä¼˜åŠ¿ï¼›çº¿ä¸ŠåŒä¼´ç¾¤ä½“å¯æä¾›è§„èŒƒåŒ–å‚ç…§ä¸æ”¯æŒï¼›å·²æœ‰å·¥ä½œç»éªŒå¯ä½œä¸ºå¯æŒ–æ˜çš„èƒ½åŠ›è¯æ®ã€‚"
    }
    
    persona_input = st.text_area(
        "ç¼–è¾‘ JSON ç”»åƒ", 
        value=json.dumps(default_persona, indent=2, ensure_ascii=False),
        height=400
    )

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# --- åŠ è½½æ¨¡å‹ ---
@st.cache_resource
def load_model(base_path, adapter_path):
    status_text = st.empty()
    status_text.info("æ­£åœ¨åŠ è½½ Tokenizer å’Œ Base Model...")
    
    tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨é‡åŒ–
    quantization_config = None
    

    model = AutoModelForCausalLM.from_pretrained(
        base_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        quantization_config=quantization_config,
        attn_implementation="flash_attention_2"  # ä½ çš„è„šæœ¬é‡Œç”¨äº† flash_attention_2
    )
    
    status_text.info(f"æ­£åœ¨åŠ è½½ LoRA Adapter: {adapter_path} ...")
    # åŠ è½½ GRPO è®­ç»ƒåçš„ Adapter
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    
    status_text.success("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, tokenizer

try:
    model, tokenizer = load_model(BASE_MODEL_PATH, ADAPTER_PATH)
except Exception as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\né”™è¯¯ä¿¡æ¯: {e}")
    st.stop()

# --- è¾…åŠ©å‡½æ•°ï¼šæ„å»º Prompt ---
def build_prompt_with_history(history, persona_json_str):
    # è¿™é‡Œå¤åˆ¶äº† prompt.py ä¸­çš„ System Prompt ç»“æ„
    system_prompt_text = f"""## è§’è‰²æ‰®æ¼”ï¼šå®¢æˆ·
ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä¸ªæ­£åœ¨æ¥å—å¿ƒç†å’¨è¯¢çš„å®¢æˆ·ã€‚

## ä»»åŠ¡æŒ‡ä»¤
ä½ å¿…é¡»ä¸¥æ ¼éµå¾ª [ç”¨æˆ·ç”»åƒ] æ¥å›åº”å’¨è¯¢å¸ˆï¼ˆè§’è‰²ä¸º 'user'ï¼‰çš„å‘è¨€ã€‚
ä½ çš„æ¯ä¸€æ¬¡å›åº”éƒ½å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ä¸‰éƒ¨åˆ†æ ¼å¼ï¼š

1.  **<thinking>...</thinking>**: é¦–å…ˆï¼Œç”Ÿæˆä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä½ ï¼ˆå®¢æˆ·ï¼‰çš„å†…å¿ƒæ´»åŠ¨ã€‚
2.  **<behavior_label>...</behavior_label>**: å…¶æ¬¡ï¼Œä»ä¸‹é¢æä¾›çš„11ä¸ªæ ‡ç­¾ä¸­ï¼Œé€‰æ‹©ä¸€ä¸ªæœ€èƒ½æè¿°ä½ æ¥ä¸‹æ¥å‘è¨€çš„æ ‡ç­¾ã€‚
3.  **å®é™…å‘è¨€**: æœ€åï¼Œå†™ä¸‹ä½ ï¼ˆå®¢æˆ·ï¼‰å®é™…è¯´å‡ºå£çš„è¯ã€‚

## è¡Œä¸ºæ ‡ç­¾ (å¿…é¡»ä»æ­¤åˆ—è¡¨ä¸­é€‰æ‹©)ï¼š
1.  ç¡®è®¤ (Confirming)
2.  æä¾›ä¿¡æ¯ (Giving Information)
3.  åˆç†è¯·æ±‚ (Reasonable Request)
4.  æ‰©å±• (Extending)
5.  é‡æ„ (Reformulating)
6.  è¡¨è¾¾å›°æƒ‘ (Expressing Confusion)
7.  é˜²å« (Defending)
8.  è‡ªæˆ‘æ‰¹è¯„æˆ–ç»æœ› (Self-criticism or Hopelessness)
9.  è½¬ç§»è¯é¢˜ (Shifting Topics)
10. ç„¦ç‚¹æ–­å¼€ (Focus Disconnection)
11. è®½åˆºæ€§å›ç­” (Sarcastic Answer)

## ä½ çš„ç”»åƒ (å¿…é¡»ä¸¥æ ¼éµå¾ª)ï¼š
{persona_json_str}"""

    # æ„å»ºå®Œæ•´çš„ Prompt å­—ç¬¦ä¸²
    full_prompt = f"<|im_start|>system\n{system_prompt_text}<|im_end|>\n"
    
    for msg in history:
        role = "user" if msg["role"] == "user" else "assistant"
        content = msg["content"]
        # æ³¨æ„ï¼šå†å²è®°å½•é‡Œå­˜çš„æ˜¯çº¯æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ Assistant çš„å®Œæ•´è¾“å‡ºï¼ˆå«æ ‡ç­¾ï¼‰æ‹¼å›å»
        full_prompt += f"<|im_start|>{role}\n{content}<|im_end|>\n"
    
    # æ·»åŠ å½“å‰çš„ User å¼•å¯¼
    full_prompt += "<|im_start|>assistant\n"
    
    return full_prompt

# --- è¾…åŠ©å‡½æ•°ï¼šè§£æè¾“å‡º ---
def parse_response(raw_text):
    """
    è§£ææ¨¡å‹è¾“å‡ºï¼Œæå– thinking, label å’Œ speech
    """
    thinking = ""
    label = ""
    speech = raw_text

    # æå– <thinking>
    think_match = re.search(r"<thinking>(.*?)</thinking>", raw_text, re.DOTALL)
    if think_match:
        thinking = think_match.group(1).strip()
        # ä» raw_text ä¸­ç§»é™¤ thinking éƒ¨åˆ†ï¼Œæ–¹ä¾¿åç»­æ˜¾ç¤º
        # speech = speech.replace(think_match.group(0), "")

    # æå– <behavior_label>
    label_match = re.search(r"<behavior_label>(.*?)</behavior_label>", raw_text, re.DOTALL)
    if label_match:
        label = label_match.group(1).strip()
        # speech = speech.replace(label_match.group(0), "")
    
    # æ¸…ç† Speechï¼šç§»é™¤æ ‡ç­¾åçš„å‰©ä½™æ–‡æœ¬å³ä¸º Speechï¼Œä½†ä¹Ÿéœ€è¦å¤„ç†å¯èƒ½æ®‹ç•™çš„æ¢è¡Œ
    # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å¤„ç†ï¼šæŠŠæ ‡ç­¾éƒ½åˆ æ‰ï¼Œå‰©ä¸‹çš„å°±æ˜¯ Speech
    clean_speech = re.sub(r"<thinking>.*?</thinking>", "", raw_text, flags=re.DOTALL)
    clean_speech = re.sub(r"<behavior_label>.*?</behavior_label>", "", clean_speech, flags=re.DOTALL)
    
    return {
        "thinking": thinking,
        "label": label,
        "speech": clean_speech.strip(),
        "raw": raw_text # ä¿å­˜åŸå§‹è¾“å‡ºç”¨äºä¸‹ä¸€æ¬¡å†å²æ‹¼æ¥
    }

# --- èŠå¤©ç•Œé¢é€»è¾‘ ---

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg["role"] == "user":
            st.markdown(msg["content"])
        else:
            # å¦‚æœæ˜¯åŠ©æ‰‹ï¼Œå°è¯•è§£æå¹¶ç¾åŒ–æ˜¾ç¤º
            parsed = parse_response(msg["content"])
            if parsed["label"]:
                st.caption(f"ğŸ·ï¸ **è¡Œä¸ºæ ‡ç­¾:** {parsed['label']}")
            if parsed["thinking"]:
                with st.expander("ğŸ’­ å†…å¿ƒæ´»åŠ¨ (Thinking)"):
                    try:
                        # å°è¯•æ ¼å¼åŒ– JSON æ˜¾ç¤º
                        think_json = json.loads(parsed["thinking"])
                        st.json(think_json)
                    except:
                        st.markdown(parsed["thinking"])
            st.markdown(parsed["speech"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¾“å…¥ä½ çš„å’¨è¯¢è¯è¯­..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. ç”Ÿæˆå›å¤
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        with st.spinner("æ¨¡æ‹Ÿæ¥è®¿è€…æ­£åœ¨æ€è€ƒ..."):
            # æ„å»º Prompt
            full_prompt_str = build_prompt_with_history(st.session_state.messages, persona_input)
            
            inputs = tokenizer(full_prompt_str, return_tensors="pt").to(model.device)
            
            # è®¾ç½® Stop Tokens (å‚è€ƒä½ çš„ trainfull.py)
            stop_words = ["USER:", "user", "USER", "ç”¨æˆ·", "<|im_end|>"]
            stop_ids = [tokenizer.convert_tokens_to_ids(w) for w in stop_words]
            # è¿‡æ»¤æ‰ unknown token
            stop_ids = [idx for idx in stop_ids if idx != tokenizer.unk_token_id]
            if tokenizer.eos_token_id not in stop_ids:
                stop_ids.append(tokenizer.eos_token_id)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=stop_ids
                )
            
            # è§£ç 
            # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            generated_ids = outputs[0][inputs.input_ids.shape[1]:]
            response_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            # è§£æå†…å®¹
            parsed = parse_response(response_text)
            
            # æ¸²æŸ“æ˜¾ç¤º
            display_content = ""
            if parsed["label"]:
                display_content += f"**ğŸ·ï¸ è¡Œä¸ºæ ‡ç­¾:** {parsed['label']}\n\n"
            
            message_placeholder.markdown(parsed["speech"]) # å…ˆæ˜¾ç¤ºä¸»è¦çš„
            
            if parsed["thinking"]:
                with st.expander("ğŸ’­ æŸ¥çœ‹å†…å¿ƒæ´»åŠ¨ (Thinking)", expanded=True):
                    try:
                        st.json(json.loads(parsed["thinking"]))
                    except:
                        st.text(parsed["thinking"])
            
            # æ›´æ–°å ä½ç¬¦ä»¥æ˜¾ç¤ºæ ‡ç­¾ + æ–‡æœ¬
            # (Streamlit çš„ expander ä¸èƒ½åµŒå¥—åœ¨ empty() æ›´æ–°é‡Œï¼Œæ‰€ä»¥ä¸Šé¢æ˜¯å³æ—¶æ¸²æŸ“ï¼Œè¿™é‡Œä¸ç”¨å†å…¨é‡è¦†ç›–)

    # 3. ä¿å­˜å®Œæ•´çš„åŸå§‹å›å¤åˆ°å†å²è®°å½•ï¼ˆä»¥ä¾¿ä¸‹ä¸€æ¬¡ Prompt æ‹¼æ¥æ—¶åŒ…å«æ ‡ç­¾å’Œ thinkingï¼‰
    st.session_state.messages.append({"role": "assistant", "content": response_text})